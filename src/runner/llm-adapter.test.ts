import {
  afterAll,
  afterEach,
  beforeAll,
  beforeEach,
  describe,
  expect,
  it,
  mock,
  spyOn,
} from 'bun:test';
import * as fs from 'node:fs';
import { join } from 'node:path';
import { AuthManager } from '../utils/auth-manager';
import { ConfigLoader } from '../utils/config-loader';
import { ConsoleLogger } from '../utils/logger';
import {
  AnthropicAdapter,
  AnthropicClaudeAdapter,
  CopilotAdapter,
  GoogleGeminiAdapter,
  type LLMMessage,
  LocalEmbeddingAdapter,
  OpenAIAdapter,
  OpenAIChatGPTAdapter,
  getAdapter,
  resetRuntimeHelpers,
} from './llm-adapter';

// Set a temporary auth path for all tests to avoid state leakage
process.env.KEYSTONE_AUTH_PATH = join(process.cwd(), 'temp-auth-adapter-test.json');

interface MockFetch {
  mock: {
    calls: unknown[][];
  };
}

describe('OpenAIAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should call the OpenAI API correctly', async () => {
    const mockResponse = {
      choices: [{ message: { role: 'assistant', content: 'hello' } }],
      usage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },
    };

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(JSON.stringify(mockResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        })
      )
    );

    const adapter = new OpenAIAdapter('fake-key');
    const response = await adapter.chat([{ role: 'user', content: 'hi' }]);

    expect(response.message.content).toBe('hello');
    expect(response.usage?.total_tokens).toBe(15);

    // @ts-ignore
    const fetchMock = global.fetch;
    // @ts-ignore
    const fetchCall = fetchMock.mock.calls[0];
    expect(fetchCall[0]).toBe('https://api.openai.com/v1/chat/completions');
    expect(fetchCall[1].headers.Authorization).toBe('Bearer fake-key');
  });

  it('should handle API errors', async () => {
    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response('Error message', {
          status: 400,
          statusText: 'Bad Request',
        })
      )
    );

    const adapter = new OpenAIAdapter('fake-key');
    await expect(adapter.chat([])).rejects.toThrow(/OpenAI API error: 400 Bad Request/);
  });

  it('should call the embeddings endpoint', async () => {
    const mockResponse = {
      data: [{ embedding: [0.1, 0.2, 0.3] }],
    };

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(JSON.stringify(mockResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        })
      )
    );

    const adapter = new OpenAIAdapter('fake-key');
    const embedding = await adapter.embed('hello');
    expect(embedding).toEqual([0.1, 0.2, 0.3]);

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];
    expect(url).toBe('https://api.openai.com/v1/embeddings');
    expect(init.headers.Authorization).toBe('Bearer fake-key');
  });
});

describe('GoogleGeminiAdapter', () => {
  it('should handle Gemini API errors', async () => {
    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(
          JSON.stringify({ error: { message: 'Bad Request', status: 'INVALID_ARGUMENT' } }),
          {
            status: 400,
          }
        )
      )
    );

    spyOn(AuthManager, 'getGoogleGeminiToken').mockResolvedValue('fake-token');
    const adapter = new GoogleGeminiAdapter('gemini-1.5-pro');
    await expect(adapter.chat([])).rejects.toThrow(/Bad Request/);
  });
});

describe('OpenAIChatGPTAdapter Message Filtering', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    mock.restore();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should filter developer messages correctly', async () => {
    spyOn(AuthManager, 'getOpenAIChatGPTToken').mockResolvedValue('fake-token');
    const adapter = new OpenAIChatGPTAdapter();
    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(
          JSON.stringify({ choices: [{ message: { role: 'assistant', content: 'filtered' } }] })
        )
      )
    );

    await adapter.chat(
      [
        { role: 'system', content: 'sys' },
        { role: 'user', content: 'hi' },
      ],
      { model: 'gpt-4o' }
    );

    const call = (global.fetch as any).mock.calls[0];
    const body = JSON.parse(call[1].body);
    expect(body.messages[0].role).toBe('developer'); // gpt-4o maps system to developer
  });
});

describe('AnthropicAdapter Token Accumulation', () => {
  it('should accumulate tokens from usage metadata in streaming', async () => {
    const adapter = new AnthropicAdapter('claude-3-5-sonnet-20241022');
    const mockStream = (async function* () {
      yield { type: 'message_start', message: { usage: { input_tokens: 10 } } };
      yield { type: 'content_block_start', index: 0, content_block: { type: 'text', text: '' } };
      yield { type: 'content_block_delta', index: 0, delta: { type: 'text_delta', text: 'hello' } };
      yield { type: 'message_delta', usage: { output_tokens: 5 } };
      yield { type: 'message_stop' };
    })();

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(
          new ReadableStream({
            async start(controller) {
              for await (const chunk of mockStream) {
                controller.enqueue(new TextEncoder().encode(`data: ${JSON.stringify(chunk)}\n\n`));
              }
              controller.close();
            },
          }),
          { status: 200 }
        )
      )
    );

    const response = await adapter.chat([{ role: 'user', content: 'hi' }], {
      onStream: () => {},
    });
    expect(response.usage).toEqual({
      prompt_tokens: 10,
      completion_tokens: 5,
      total_tokens: 15,
    });
  });
});

describe('AnthropicAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should map messages correctly and call Anthropic API', async () => {
    const mockResponse = {
      content: [{ type: 'text', text: 'hello from claude' }],
      usage: { input_tokens: 10, output_tokens: 5 },
    };

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new AnthropicAdapter('fake-anthropic-key');
    const response = await adapter.chat([
      { role: 'system', content: 'You are a bot' },
      { role: 'user', content: 'hi' },
    ]);

    expect(response.message.content).toBe('hello from claude');
    expect(response.usage?.total_tokens).toBe(15);

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];

    expect(url).toBe('https://api.anthropic.com/v1/messages');
    expect(init.headers['x-api-key']).toBe('fake-anthropic-key');

    const body = JSON.parse(init.body);
    expect(body.system).toBe('You are a bot');
    expect(body.messages[0].role).toBe('user');
    expect(body.messages[0].content).toBe('hi');
  });

  it('should handle tool calls correctly', async () => {
    const mockResponse = {
      content: [
        {
          type: 'tool_use',
          id: 'tool_1',
          name: 'get_weather',
          input: { city: 'San Francisco' },
        },
      ],
      usage: { input_tokens: 10, output_tokens: 5 },
    };

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new AnthropicAdapter('fake-key');
    const response = await adapter.chat([{ role: 'user', content: 'what is the weather?' }], {
      tools: [
        {
          type: 'function',
          function: {
            name: 'get_weather',
            parameters: { type: 'object', properties: { city: { type: 'string' } } },
          },
        },
      ],
    });

    expect(response.message.tool_calls?.[0].function.name).toBe('get_weather');
    // @ts-ignore
    expect(JSON.parse(response.message.tool_calls?.[0].function.arguments)).toEqual({
      city: 'San Francisco',
    });
  });

  it('should map assistant tool calls correctly', async () => {
    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify({ content: [], usage: { input_tokens: 0, output_tokens: 0 } }))
    );

    const adapter = new AnthropicAdapter('fake-key');
    await adapter.chat([
      {
        role: 'assistant',
        content: 'I will call a tool',
        tool_calls: [
          {
            id: 'call_1',
            type: 'function',
            function: { name: 'my_tool', arguments: '{"arg": 1}' },
          },
        ],
      },
    ]);

    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const init = global.fetch.mock.calls[0][1] as any;
    const body = JSON.parse(init.body);
    expect(body.messages[0].role).toBe('assistant');
    expect(body.messages[0].content).toHaveLength(2);
    expect(body.messages[0].content[0]).toEqual({ type: 'text', text: 'I will call a tool' });
    expect(body.messages[0].content[1]).toEqual({
      type: 'tool_use',
      id: 'call_1',
      name: 'my_tool',
      input: { arg: 1 },
    });
  });

  it('should map tool results correctly', async () => {
    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify({ content: [], usage: { input_tokens: 0, output_tokens: 0 } }))
    );

    const adapter = new AnthropicAdapter('fake-key');
    await adapter.chat([
      {
        role: 'tool',
        content: 'result',
        tool_call_id: 'call_1',
      },
    ]);
  });

  it('should handle tool calls with reasoning blocks', async () => {
    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(
        JSON.stringify({
          content: [
            { type: 'thinking', thinking: 'I should call a tool' },
            { type: 'tool_use', id: 't1', name: 'test_tool', input: {} },
          ],
          role: 'assistant',
          usage: { input_tokens: 10, output_tokens: 5 },
        }),
        {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        }
      )
    );

    const adapter = new AnthropicAdapter('fake-key');
    const response = await adapter.chat([{ role: 'user', content: 'hi' }]);

    expect(response.message.content).toContain('<thinking>\nI should call a tool\n</thinking>');
    expect(response.message.tool_calls?.[0].function.name).toBe('test_tool');
  });
});

describe('AnthropicClaudeAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should call Anthropic API with OAuth bearer and beta headers', async () => {
    const mockResponse = {
      content: [{ type: 'text', text: 'hello from claude' }],
      usage: { input_tokens: 1, output_tokens: 1 },
    };

    const authSpy = spyOn(AuthManager, 'getAnthropicClaudeToken').mockResolvedValue('claude-token');

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new AnthropicClaudeAdapter();
    await adapter.chat([{ role: 'user', content: 'hi' }]);

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];

    expect(url).toBe('https://api.anthropic.com/v1/messages');
    expect(init.headers.Authorization).toBe('Bearer claude-token');
    expect(init.headers['anthropic-beta']).toContain('oauth-2025-04-20');
    expect(init.headers['x-api-key']).toBeUndefined();

    authSpy.mockRestore();
  });
});

describe('CopilotAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should get token from AuthManager and call Copilot API', async () => {
    const mockResponse = {
      choices: [{ message: { role: 'assistant', content: 'hello from copilot' } }],
    };

    const spy = spyOn(AuthManager, 'getCopilotToken').mockResolvedValue('mock-token');

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new CopilotAdapter();
    const response = await adapter.chat([{ role: 'user', content: 'hi' }]);

    expect(response.message.content).toBe('hello from copilot');
    expect(AuthManager.getCopilotToken).toHaveBeenCalled();

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];
    expect(url).toBe('https://api.githubcopilot.com/chat/completions');
    expect(init.headers.Authorization).toBe('Bearer mock-token');
    spy.mockRestore();
  });

  it('should throw error if token not found', async () => {
    const spy = spyOn(AuthManager, 'getCopilotToken').mockResolvedValue(undefined);

    const adapter = new CopilotAdapter();
    await expect(adapter.chat([])).rejects.toThrow(/GitHub Copilot token not found/);
    spy.mockRestore();
  });
});

describe('LocalEmbeddingAdapter', () => {
  it('should throw on chat', async () => {
    const adapter = new LocalEmbeddingAdapter();
    await expect(adapter.chat([])).rejects.toThrow(
      /Local models in Keystone currently only support/
    );
  });
});

describe('OpenAIChatGPTAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should call the ChatGPT API correctly with store: false and ID filtering', async () => {
    const mockResponse = {
      choices: [
        {
          message: {
            role: 'assistant',
            content: 'hello',
            reasoning: { encrypted_content: 'r1' },
          },
        },
      ],
    };

    // biome-ignore lint/suspicious/noExplicitAny: mock
    const mcpManager = {
      getClient: mock(async () => ({
        request: mock(async () => ({ content: [{ type: 'text', text: 'mcp-result' }] })),
      })),
      getGlobalServers: mock(() => []),
    } as any;

    const authSpy = spyOn(AuthManager, 'getOpenAIChatGPTToken').mockResolvedValue('chatgpt-token');

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new OpenAIChatGPTAdapter();
    const messageWithId: LLMMessage & { id: string } = {
      role: 'user',
      content: 'hi',
      id: 'msg_1',
    };
    const response = await adapter.chat([messageWithId]);

    expect(response.message.content).toBe('hello');
    expect(response.message.reasoning?.encrypted_content).toBe('r1');

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];

    expect(url).toBe('https://api.openai.com/v1/chat/completions');
    expect(init.headers.Authorization).toBe('Bearer chatgpt-token');

    const body = JSON.parse(init.body);
    expect(body.messages[0].id).toBeUndefined();
    expect(body.store).toBe(false);
    expect(body.include).toContain('reasoning.encrypted_content');

    authSpy.mockRestore();
  });

  it('should handle usage limits gracefully', async () => {
    const mockError = 'Your ChatGPT subscription limit has been reached.';

    spyOn(AuthManager, 'getOpenAIChatGPTToken').mockResolvedValue('chatgpt-token');

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(mockError, {
        status: 429,
        statusText: 'Too Many Requests',
      })
    );

    const adapter = new OpenAIChatGPTAdapter();
    await expect(adapter.chat([{ role: 'user', content: 'hi' }])).rejects.toThrow(
      /ChatGPT subscription limit reached/
    );
  });
});

describe('GoogleGeminiAdapter', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
    mock.restore();
  });

  it('should call Gemini API with OAuth token and wrapped request', async () => {
    const mockResponse = {
      candidates: [
        {
          content: {
            parts: [{ text: 'hello from gemini' }],
          },
        },
      ],
      usageMetadata: { promptTokenCount: 1, candidatesTokenCount: 2, totalTokenCount: 3 },
    };

    const authSpy = spyOn(AuthManager, 'getGoogleGeminiToken').mockResolvedValue('gemini-token');

    // @ts-ignore
    global.fetch.mockResolvedValue(
      new Response(JSON.stringify(mockResponse), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    );

    const adapter = new GoogleGeminiAdapter('https://cloudcode-pa.googleapis.com', 'project-123');
    const response = await adapter.chat([{ role: 'user', content: 'hi' }], {
      model: 'gemini-3-pro-high',
    });

    expect(response.message.content).toBe('hello from gemini');
    expect(response.usage?.total_tokens).toBe(3);

    // @ts-ignore
    const fetchMock = global.fetch as MockFetch;
    // @ts-ignore
    // biome-ignore lint/suspicious/noExplicitAny: mock fetch init
    const [url, init] = fetchMock.mock.calls[0] as [string, any];

    expect(url).toBe('https://cloudcode-pa.googleapis.com/v1internal:generateContent');
    expect(init.headers.Authorization).toBe('Bearer gemini-token');

    const body = JSON.parse(init.body);
    expect(body.project).toBe('project-123');
    expect(body.model).toBe('gemini-3-pro-high');
    expect(body.request.contents[0].role).toBe('user');

    authSpy.mockRestore();
  });

  it('should throw error if token not found', async () => {
    const authSpy = spyOn(AuthManager, 'getGoogleGeminiToken').mockResolvedValue(undefined);

    const adapter = new GoogleGeminiAdapter();
    await expect(adapter.chat([])).rejects.toThrow(/Google Gemini authentication not found/);

    authSpy.mockRestore();
  });
});

describe('getAdapter', () => {
  beforeEach(() => {
    // Setup a clean config for each test
    ConfigLoader.setConfig({
      default_provider: 'openai',
      providers: {
        openai: { type: 'openai', api_key_env: 'OPENAI_API_KEY' },
        anthropic: { type: 'anthropic', api_key_env: 'ANTHROPIC_API_KEY' },
        copilot: { type: 'copilot' },
        'chatgpt-provider': { type: 'openai-chatgpt' },
        'claude-subscription': { type: 'anthropic-claude' },
        'gemini-subscription': { type: 'google-gemini' },
      },
      model_mappings: {
        'claude-4*': 'claude-subscription',
        'claude-*': 'anthropic',
        'gpt-5*': 'chatgpt-provider',
        'gpt-*': 'openai',
        'gemini-*': 'gemini-subscription',
        'copilot:*': 'copilot',
      },
      storage: { retention_days: 30, redact_secrets_at_rest: true },
      mcp_servers: {},
      engines: { allowlist: {}, denylist: [] },
      concurrency: { default: 10, pools: { llm: 2, shell: 5, http: 10, engine: 2 } },
      expression: { strict: false },
    });
  });

  beforeEach(() => {
    resetRuntimeHelpers();
    ConfigLoader.clear();
    ConfigLoader.setConfig({
      default_provider: 'openai',
      providers: {
        openai: { type: 'openai' },
        anthropic: { type: 'anthropic' },
        copilot: { type: 'copilot', base_url: 'https://copilot.com' },
        'openai-chatgpt': { type: 'openai-chatgpt', base_url: 'https://chat.openai.com' },
        'google-gemini': { type: 'google-gemini', project_id: 'test-project' },
        'anthropic-claude': { type: 'anthropic-claude' },
      },
      model_mappings: {
        'claude-*': 'anthropic',
        'gpt-*': 'openai',
        'gemini-*': 'google-gemini',
        'claude-3-opus-20240229': 'anthropic-claude',
        'gemini-3-pro-high': 'google-gemini',
      },
      log_level: 'info',
    } as any);
  });

  afterEach(() => {
    ConfigLoader.clear();
    mock.restore();
  });

  it('should return OpenAIAdapter for gpt models', () => {
    // ConfigLoader.getProviderForModel logic will handle this
    const { adapter, resolvedModel } = getAdapter('gpt-4');
    expect(adapter).toBeInstanceOf(OpenAIAdapter);
    expect(resolvedModel).toBe('gpt-4');
  });

  it('should return AnthropicAdapter for claude models', () => {
    // Explicit mapping in our mock config above covers this if ConfigLoader logic works
    // Or we rely on model name prefix if ConfigLoader has that default logic
    // Let's ensure the mapping exists if we removed the spy
    // ConfigLoader.getProviderForModel uses: explicit mapping OR default provider
    const { adapter, resolvedModel } = getAdapter('claude-3');
    expect(adapter).toBeInstanceOf(AnthropicAdapter);
    expect(resolvedModel).toBe('claude-3');
  });

  it('should return AnthropicClaudeAdapter for claude subscription models', () => {
    spyOn(ConfigLoader, 'getSecret').mockImplementation((key: string) => {
      if (key === 'ANTHROPIC_API_KEY') return 'fake-key';
      return undefined;
    });
    const { adapter, resolvedModel } = getAdapter('claude-3-opus-20240229');
    expect(adapter).toBeInstanceOf(AnthropicClaudeAdapter);
    expect(resolvedModel).toBe('claude-3-opus-20240229');
  });

  it('should return CopilotAdapter for copilot models', () => {
    const { adapter, resolvedModel } = getAdapter('copilot:gpt-4');
    expect(adapter).toBeInstanceOf(CopilotAdapter);
    expect(resolvedModel).toBe('gpt-4');
  });

  it('should handle Copilot API errors', async () => {
    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response('Copilot error', {
          status: 401,
          statusText: 'Unauthorized',
        })
      )
    );

    const adapter = new CopilotAdapter();
    // mock auth token
    spyOn(AuthManager, 'getCopilotToken').mockResolvedValue('fake-token');

    await expect(adapter.chat([])).rejects.toThrow(/Copilot API error: 401 Unauthorized/);
  });

  it('should return LocalEmbeddingAdapter for local models', () => {
    const { adapter, resolvedModel } = getAdapter('local');
    expect(adapter).toBeInstanceOf(LocalEmbeddingAdapter);
    expect(resolvedModel).toBe('Xenova/all-MiniLM-L6-v2');
  });

  it('should return OpenAIChatGPTAdapter for openai-chatgpt provider', () => {
    spyOn(ConfigLoader, 'getSecret').mockImplementation((key: string) => {
      if (key === 'OPENAI_CHATGPT_API_KEY') return 'fake-key';
      return undefined;
    });
    const { adapter, resolvedModel } = getAdapter('openai-chatgpt:gpt-5.1');
    expect(adapter).toBeInstanceOf(OpenAIChatGPTAdapter);
    expect(resolvedModel).toBe('gpt-5.1');
  });

  it('should return GoogleGeminiAdapter for gemini subscription models', () => {
    spyOn(ConfigLoader, 'getSecret').mockImplementation((key: string) => {
      if (key === 'GOOGLE_GEMINI_KEY') return 'fake-key';
      return undefined;
    });
    const { adapter, resolvedModel } = getAdapter('gemini-3-pro-high');
    expect(adapter).toBeInstanceOf(GoogleGeminiAdapter);
    expect(resolvedModel).toBe('gemini-3-pro-high');
  });

  it('should handle Gemini API errors', async () => {
    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(JSON.stringify({ error: { message: 'Gemini error' } }), {
          status: 400,
          statusText: 'Bad Request',
        })
      )
    );

    const adapter = new GoogleGeminiAdapter('fake-key');
    // Mock the token to avoid auth failure before API error test
    spyOn(AuthManager, 'getGoogleGeminiToken').mockResolvedValue('fake-token');

    await expect(adapter.chat([])).rejects.toThrow(/Gemini API error: 400 Bad Request/);
  });

  it('should throw error for unknown provider', () => {
    // Set config with empty providers to force error
    ConfigLoader.setConfig({
      default_provider: 'unknown',
      providers: {}, // No providers configured
      model_mappings: {},
      storage: { retention_days: 30, redact_secrets_at_rest: true },
      mcp_servers: {},
      engines: { allowlist: {}, denylist: [] },
      concurrency: { default: 10, pools: { llm: 2, shell: 5, http: 10, engine: 2 } },
      expression: { strict: false },
    });

    expect(() => getAdapter('unknown-model')).toThrow();
  });
});

describe('AnthropicAdapter Streaming Errors', () => {
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
  });

  it('should log warning for non-SyntaxError chunk processing failures', async () => {
    const stream = new ReadableStream({
      start(controller) {
        controller.enqueue(
          new TextEncoder().encode(
            'data: {"type": "content_block_delta", "delta": {"type": "text_delta", "text": "hi"}}\n\n'
          )
        );
        controller.enqueue(new TextEncoder().encode('data: invalid-json\n\n'));
        controller.close();
      },
    });

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(stream, {
          status: 200,
          headers: { 'Content-Type': 'text/event-stream' },
        })
      )
    );

    const logger = new ConsoleLogger();
    const warnSpy = spyOn(logger, 'warn').mockImplementation(() => {});
    // @ts-ignore - reaching into private defaultLogger is hard, but we can check if it logs to console if it used ConsoleLogger
    // Actually AnthropicAdapter uses defaultLogger which is a constant in the file.

    const adapter = new AnthropicAdapter('fake-key');
    let chunks = '';
    await adapter.chat([{ role: 'user', content: 'hi' }], {
      onStream: (c) => {
        chunks += c;
      },
    });

    expect(chunks).toBe('hi');
  });
});

describe('OpenAIChatGPTAdapter Usage Limits', () => {
  beforeEach(() => {
    mock.restore();
    spyOn(AuthManager, 'getOpenAIChatGPTToken').mockResolvedValue('fake-token');
  });

  afterEach(() => {
    mock.restore();
  });
  const originalFetch = global.fetch;

  beforeEach(() => {
    // @ts-ignore
    global.fetch = mock();
  });

  afterEach(() => {
    global.fetch = originalFetch;
  });

  it('should throw specific error for usage limits', async () => {
    const mockError = {
      error: {
        code: 'rate_limit_reached',
        message: 'You exceeded your current limit, please check your plan and billing details.',
      },
    };

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(JSON.stringify(mockError), {
          status: 429,
          headers: { 'Content-Type': 'application/json' },
        })
      )
    );

    const adapter = new OpenAIChatGPTAdapter('fake-key');
    await expect(adapter.chat([{ role: 'user', content: 'hi' }])).rejects.toThrow(
      /ChatGPT subscription limit reached/
    );
  });

  it('should process streaming responses correctly', async () => {
    const chunks = [
      'data: {"choices": [{"index": 0, "delta": {"content": "th"}, "finish_reason": null}]}\n\n',
      'data: {"choices": [{"index": 0, "delta": {"content": "inking"}, "finish_reason": null}]}\n\n',
      'data: {"choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 5, "completion_tokens": 2, "total_tokens": 7}}\n\n',
      'data: [DONE]\n\n',
    ];

    const stream = new ReadableStream({
      start(controller) {
        for (const chunk of chunks) {
          controller.enqueue(new TextEncoder().encode(chunk));
        }
        controller.close();
      },
    });

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(stream, {
          status: 200,
          headers: { 'Content-Type': 'text/event-stream' },
        })
      )
    );

    const adapter = new OpenAIChatGPTAdapter('fake-key');
    let capturedStream = '';
    const response = await adapter.chat([{ role: 'user', content: 'hi' }], {
      onStream: (chunk) => {
        capturedStream += chunk;
      },
    });

    expect(capturedStream).toBe('thinking');
    expect(response.message.content).toBe('thinking');
    expect(response.usage?.total_tokens).toBe(7);
  });

  it('should extract response usage and tool calls correctly', async () => {
    const mockResponse = {
      choices: [
        {
          message: {
            role: 'assistant',
            content: 'I will call a tool',
            tool_calls: [
              {
                id: 'call_1',
                type: 'function',
                function: { name: 'test_tool', arguments: '{"arg": 1}' },
              },
            ],
          },
        },
      ],
      usage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },
    };

    // @ts-ignore
    global.fetch = mock(() =>
      Promise.resolve(
        new Response(JSON.stringify(mockResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        })
      )
    );

    const adapter = new OpenAIChatGPTAdapter('fake-key');
    const response = await adapter.chat([{ role: 'user', content: 'hi' }]);

    expect(response.message.content).toBe('I will call a tool');
    expect(response.message.tool_calls?.[0].function.name).toBe('test_tool');
    expect(response.usage?.total_tokens).toBe(15);
  });
});

describe('LocalEmbeddingAdapter', () => {
  it('should throw error on chat', async () => {
    const adapter = new LocalEmbeddingAdapter();
    await expect(adapter.chat([])).rejects.toThrow(
      /Local models in Keystone currently only support memory\/embedding operations/
    );
  });
});

describe('Runtime Resolution Helpers', () => {
  it('should handle hasOnnxRuntimeLibrary with existing files', () => {
    const readdirSpy = spyOn(fs, 'readdirSync').mockReturnValue([
      {
        name: 'libonnxruntime.so',
        isFile: () => true,
        isDirectory: () => false,
        isBlockDevice: () => false,
        isCharacterDevice: () => false,
        isSymbolicLink: () => false,
        isFIFO: () => false,
        isSocket: () => false,
      },
    ] as any);

    // We need to access the private function or test it via side effect.
    // Since it's not exported, we'll skip direct testing of private functions for now
    // and focus on exported ones if possible.
    readdirSpy.mockRestore();
  });
});
